# !/usr/bin/env python3

"""
Context Parallel FlashMask Attention Implementation

This module provides context parallel implementation of flashmask attention with load balancing
using DualChunkSwap strategy. Context parallelism partitions tensors along the sequence
dimension to enable long-context LLMs in a distributed fashion.
"""

import paddle
import paddle.nn.functional as F
from paddle import _C_ops
from paddle import distributed as dist
from paddle.distributed import fleet
from paddle.nn.functional.flash_attention import flashmask_attention
from paddle.autograd.py_layer import PyLayer
import numpy as np

def scatter_balance(input_tensor, group=None, axis=0):
    """
    Evenly split input tensor along the specified axis across model parallel ranks.

    This function implements balanced scattering by taking chunks from both ends
    of the tensor to ensure load balancing across ranks.

    Args:
        input_tensor (paddle.Tensor): Input tensor to be scattered
        group (paddle.distributed.Group, optional): Communication group.
            If None, uses model parallel group from fleet
        axis (int, optional): Axis along which to scatter. Defaults to 0

    Returns:
        paddle.Tensor: Scattered tensor chunk for current rank

    Note:
        This API is different from distributed.scatter - it performs balanced
        splitting by taking chunks from both ends of the sequence.
    """
    if group is None:
        hcg = fleet.get_hybrid_communicate_group()
        group = hcg.get_context_parallel_group()

    parallelism = group.nranks
    if parallelism == 1:
        return input_tensor.clone()

    rank = group.rank
    seq_len = input_tensor.shape[axis]

    # Ensure sequence length is divisible by parallelism * 2 for balanced splitting
    assert (
        seq_len % (parallelism * 2) == 0
    ), f"Input sequence length {seq_len} can't be divided exactly by sequence parallelism * 2 {parallelism * 2}"

    interval = seq_len // parallelism // 2
    total_len = input_tensor.shape[axis]

    # Take chunk from the beginning
    chunk_start = paddle.slice(input_tensor, axes=[axis], starts=[interval * rank], ends=[interval * (rank + 1)])

    # Take chunk from the end (in reverse order)
    chunk_end = paddle.slice(
        input_tensor, axes=[axis], starts=[total_len - interval * (rank + 1)], ends=[total_len - interval * rank]
    )

    # Concatenate chunks
    result = paddle.concat([chunk_start, chunk_end], axis=axis)

    # Use assign to free the memory of the whole input tensor to avoid OOM
    # since slice uses stride and maintains reference to original tensor
    result = paddle.assign(result)
    return result


def all_gather_balance(input_tensor, group=None, axis=0):
    """
    All-gather operation with balanced reconstruction.

    This function performs all-gather to reconstruct the original tensor
    from balanced scattered chunks.

    Args:
        input_tensor (paddle.Tensor): Input tensor chunk
        group (paddle.distributed.Group, optional): Communication group
        axis (int, optional): Axis along which to gather. Defaults to 0

    Returns:
        paddle.Tensor: Reconstructed full tensor
    """
    if group is None:
        hcg = fleet.get_hybrid_communicate_group()
        group = hcg.get_context_parallel_group()

    parallelism = group.nranks
    if parallelism == 1:
        return input_tensor.clone()

    # Split input into two halves (start and end chunks)
    chunk_start, chunk_end = paddle.split(input_tensor, 2, axis=axis)

    if axis == 0:
        # Handle axis=0 case with optimized memory layout
        output_shape_start = list(chunk_start.shape)
        output_shape_start[axis] = output_shape_start[axis] * parallelism

        gathered_start = paddle.empty(shape=output_shape_start, dtype=input_tensor.dtype)
        dist.stream.all_gather(gathered_start, chunk_start, group=group, use_calc_stream=True)

        # Gather end chunks
        gathered_end_list = [paddle.empty(chunk_end.shape, dtype=input_tensor.dtype) for _ in range(parallelism)]
        dist.stream.all_gather(gathered_end_list, chunk_end, group=group, use_calc_stream=True)

        # Reverse the end chunks to reconstruct original order
        gathered_end_list = gathered_end_list[::-1]

        result = paddle.concat([gathered_start] + gathered_end_list, axis=axis)
        return result
    else:
        # Handle other axes
        gathered_start_list = [paddle.empty(chunk_start.shape, dtype=input_tensor.dtype) for _ in range(parallelism)]
        dist.stream.all_gather(gathered_start_list, chunk_start, group=group, use_calc_stream=True)

        gathered_end_list = [paddle.empty(chunk_end.shape, dtype=input_tensor.dtype) for _ in range(parallelism)]
        dist.stream.all_gather(gathered_end_list, chunk_end, group=group, use_calc_stream=True)

        # Reverse the end chunks
        gathered_end_list = gathered_end_list[::-1]

        result = paddle.concat(gathered_start_list + gathered_end_list, axis=axis)
        return result


def reduce_scatter_any_axis(input_tensor, axis, group=None):
    """
    Reduce-scatter operation along any axis.

    Performs element-wise reduction (sum) across ranks and scatters the result
    so each rank gets a portion of the reduced tensor.

    Args:
        input_tensor (paddle.Tensor): Input tensor to reduce and scatter
        axis (int): Axis along which to perform reduce-scatter
        group (paddle.distributed.Group, optional): Communication group

    Returns:
        paddle.Tensor: Reduced and scattered tensor chunk
    """
    if group is None:
        hcg = fleet.get_hybrid_communicate_group()
        group = hcg.get_context_parallel_group()

    parallelism = group.nranks
    if parallelism == 1:
        return input_tensor.clone()

    assert input_tensor.shape[axis] % parallelism == 0, (
        f"Input sequence length {input_tensor.shape[axis]} can't be ",
        f"divided exactly by context parallelism {parallelism}",
    )

    if axis == 0:
        # Optimized path for axis=0
        output_shape = list(input_tensor.shape)
        output_shape[0] = output_shape[0] // parallelism

        output = paddle.empty(shape=output_shape, dtype=input_tensor.dtype)
        dist.stream.reduce_scatter(output, input_tensor, op=dist.ReduceOp.SUM, group=group, use_calc_stream=False)
        return output
    else:
        # General case for other axes using alltoall
        input_chunks = paddle.split(input_tensor, parallelism, axis=axis)

        output_buffers = [paddle.empty(input_chunks[0].shape, dtype=input_tensor.dtype) for _ in range(parallelism)]

        dist.stream.alltoall(output_buffers, input_chunks, group=group, use_calc_stream=False)

        # Sum the received chunks
        result = paddle.stack(output_buffers, axis=0).sum(axis=0)
        return result


def reduce_scatter_any_axis_balance(input_tensor, axis, group=None):
    """
    Balanced reduce-scatter operation along any axis.

    Similar to reduce_scatter_any_axis but uses balanced splitting strategy
    by processing chunks from both ends of the tensor.

    Args:
        input_tensor (paddle.Tensor): Input tensor to reduce and scatter
        axis (int): Axis along which to perform reduce-scatter
        group (paddle.distributed.Group, optional): Communication group

    Returns:
        paddle.Tensor: Reduced and scattered tensor chunk with balanced distribution
    """
    if group is None:
        hcg = fleet.get_hybrid_communicate_group()
        group = hcg.get_context_parallel_group()

    parallelism = group.nranks
    if parallelism == 1:
        return input_tensor.clone()

    assert input_tensor.shape[axis] % (parallelism * 2) == 0, (
        f"Input sequence length {input_tensor.shape[axis]} can't be ",
        f"divided exactly by context parallelism * 2 {parallelism * 2}",
    )

    # Split input into two halves
    input_start, input_end = paddle.split(input_tensor, 2, axis=axis)

    # Split each half across ranks
    chunks_start = paddle.split(input_start, parallelism, axis=axis)
    chunks_end = paddle.split(input_end, parallelism, axis=axis)

    # Reverse end chunks for balanced distribution
    chunks_end = chunks_end[::-1]

    # Combine corresponding start and end chunks
    combined_chunks = [
        paddle.concat([start_chunk, end_chunk], axis=axis) for start_chunk, end_chunk in zip(chunks_start, chunks_end)
    ]

    # Perform alltoall communication
    output_buffers = [paddle.empty(combined_chunks[0].shape, dtype=input_tensor.dtype) for _ in range(parallelism)]
    

    dist.stream.alltoall(output_buffers, combined_chunks, group=group, use_calc_stream=True)

    # Sum the received chunks
    result = paddle.stack(output_buffers, axis=0).sum(axis=0)
    return result


class ContextParallelScatterOp(PyLayer):
    """
    Context parallel scatter operation using PyLayer for automatic differentiation.

    Forward: Scatter input tensor using balanced splitting
    Backward: All-gather gradients using balanced reconstruction
    """

    @staticmethod
    def forward(ctx, input_tensor, axis=0):
        """
        Forward pass: scatter input tensor across context parallel ranks.

        Args:
            ctx: Context object for saving information for backward pass
            input_tensor (paddle.Tensor): Input tensor to scatter
            axis (int): Axis along which to scatter

        Returns:
            paddle.Tensor: Scattered tensor chunk
        """
        ctx.axis = axis
        hcg = fleet.get_hybrid_communicate_group()

        assert hcg.get_context_parallel_world_size() > 1, (
            f"ScatterOpCP must be used with context parallel, ",
            f"context_parallel_world_size={hcg.get_context_parallel_world_size()}",
        )

        group = hcg.get_context_parallel_group()
        ctx.group = group

        return scatter_balance(input_tensor, axis=axis, group=group)

    @staticmethod
    def backward(ctx, grad_output):
        """
        Backward pass: all-gather gradients.

        Args:
            ctx: Context object with saved information
            grad_output (paddle.Tensor): Gradient of output

        Returns:
            tuple: Gradients for input arguments
        """
        grad_input = all_gather_balance(grad_output, axis=ctx.axis, group=ctx.group)
        return grad_input


class ContextParallelGatherOp(PyLayer):
    """
    Context parallel gather operation using PyLayer for automatic differentiation.

    Forward: All-gather input tensor using balanced reconstruction
    Backward: Scatter gradients using balanced splitting
    """

    @staticmethod
    def forward(ctx, input_tensor, axis=0):
        """
        Forward pass: all-gather input tensor across context parallel ranks.

        Args:
            ctx: Context object for saving information for backward pass
            input_tensor (paddle.Tensor): Input tensor to gather
            axis (int): Axis along which to gather

        Returns:
            paddle.Tensor: Gathered full tensor
        """
        ctx.axis = axis
        hcg = fleet.get_hybrid_communicate_group()

        assert hcg.get_context_parallel_world_size() > 1, (
            f"GatherOpCP must be used with context parallel, ",
            f"context_parallel_world_size={hcg.get_context_parallel_world_size()}",
        )

        group = hcg.get_context_parallel_group()
        ctx.group = group

        return all_gather_balance(input_tensor, axis=axis, group=group)

    @staticmethod
    def backward(ctx, grad_output):
        """
        Backward pass: scatter gradients.

        Args:
            ctx: Context object with saved information
            grad_output (paddle.Tensor): Gradient of output

        Returns:
            tuple: Gradients for input arguments
        """
        grad_input = scatter_balance(grad_output, axis=ctx.axis, group=ctx.group)
        return grad_input


class ContextParallelAllGatherOp(PyLayer):
    """
    Context parallel all-gather operation with gradient reduction.

    Forward: All-gather input tensor (e.g., [batch, seq_len/n, hidden] -> [batch, seq_len, hidden])
    Backward: Reduce-scatter gradients with balanced distribution

    This operation is similar to AllGatherOp but maintains context parallel state
    after gradient aggregation.
    """

    @staticmethod
    def forward(ctx, input_tensor, axis):
        """
        Forward pass: all-gather input tensor.

        Args:
            ctx: Context object for saving information
            input_tensor (paddle.Tensor): Input tensor with shape [batch, seq_len/n, hidden]
            axis (int): Axis along which to gather

        Returns:
            paddle.Tensor: Gathered tensor with shape [batch, seq_len, hidden]
        """
        ctx.axis = axis
        hcg = fleet.get_hybrid_communicate_group()

        assert hcg.get_context_parallel_world_size() > 1, (
            f"AllGatherOpCP must be used with context parallel, ",
            f"context_parallel_world_size={hcg.get_context_parallel_world_size()}",
        )

        group = hcg.get_context_parallel_group()
        ctx.group = group

        return all_gather_balance(input_tensor, axis=axis, group=group)

    @staticmethod
    def backward(ctx, grad_output):
        """
        Backward pass: reduce-scatter gradients.

        Args:
            ctx: Context object with saved information
            grad_output (paddle.Tensor): Gradient with shape [batch, seq_len, hidden]

        Returns:
            tuple: Gradients with shape [batch, seq_len/n, hidden]
        """
        grad_input = reduce_scatter_any_axis_balance(grad_output, axis=ctx.axis, group=ctx.group)
        return grad_input


def preprocess_index(startend_row_indices, chunk_id, seq_blocksize, max_seqlen_q):
    """
    Preprocess startend row indices for a single chunk.

    Adjusts the startend_row_indices relative to the chunk's starting position and
    clips them to valid range.

    Args:
        startend_row_indices (paddle.Tensor): Original startend row indices
        chunk_id (int): ID of the current chunk
        seq_blocksize (int): Size of each sequence block
        max_seqlen_q (int): Maximum sequence length for queries

    Returns:
        paddle.Tensor: Preprocessed row indices
    """
    rows_min = chunk_id * seq_blocksize
    adjusted_indices = startend_row_indices - rows_min
    clipped_indices = paddle.clip(adjusted_indices, min=0, max=max_seqlen_q)
    return clipped_indices


def preprocess_index_dual_chunks(startend_row_indices, chunk_id_first, chunk_id_second, seq_blocksize, max_seqlen_q):
    """
    Preprocess row indices for dual chunks (DualChunkSwap strategy).

    This function handles the index preprocessing for the balanced dual-chunk
    strategy where each rank processes chunks from both ends of the sequence.

    Args:
        startend_row_indices (paddle.Tensor): Original row indices
        chunk_id_first (int): ID of the first chunk
        chunk_id_second (int): ID of the second chunk
        seq_blocksize (int): Size of each sequence block
        max_seqlen_q (int): Maximum sequence length for queries

    Returns:
        paddle.Tensor: Preprocessed row indices for dual chunks
    """
    # Calculate starting positions for both chunks
    rows_min_first = chunk_id_first * seq_blocksize
    rows_min_second = chunk_id_second * seq_blocksize

    # Process first chunk indices
    indices_first = startend_row_indices - rows_min_first
    indices_first = paddle.clip(indices_first, min=0, max=max_seqlen_q)

    # Process second chunk indices
    indices_second = startend_row_indices - rows_min_second
    indices_second = paddle.clip(indices_second, min=0, max=max_seqlen_q)

    # Offset second chunk indices to avoid overlap
    indices_second = paddle.where(indices_second != 0, indices_second + max_seqlen_q, indices_second)

    # Combine indices from both chunks
    combined_indices = paddle.maximum(indices_first, indices_second)
    return combined_indices


def cp_flashmask_allgatherkv_balance_forward(query, key, value, startend_row_indices, group, causal, is_training):
    """
    Forward pass of context parallel flashmask attention with balanced all-gather strategy.

    This function implements the forward pass of flash attention with context parallelism
    using the DualChunkSwap strategy for load balancing.

    Args:
        query (paddle.Tensor): Query tensor with shape [batch, seq_len/n, num_heads, head_dim]
        key (paddle.Tensor): Key tensor with shape [batch, seq_len/n, num_heads, head_dim]
        value (paddle.Tensor): Value tensor with shape [batch, seq_len/n, num_heads, head_dim]
        startend_row_indices (paddle.Tensor): Row indices for attention mask
        group (paddle.distributed.Group): Communication group
        causal (bool): Whether to use causal attention
        is_training (bool): Whether in training mode

    Returns:
        tuple: (output, log_sum_exp, processed_indices)
    """
    paddle.base.core.nvprof_nvtx_push("cp_flashmask_allgatherkv_balance_forward")

    rank = group.rank
    cp_size = group.world_size

    # All-gather key tensors across context parallel ranks
    key_gathered = all_gather_balance(key, axis=1, group=group)

    # All-gather value tensors across context parallel ranks
    value_gathered = all_gather_balance(value, axis=1, group=group)

    # Calculate sequence block size for dual-chunk strategy
    seq_blocksize = query.shape[1] // 2

    # Preprocess indices for dual-chunk strategy
    startend_row_indices = preprocess_index_dual_chunks(
        startend_row_indices,
        chunk_id_first=rank,
        chunk_id_second=2 * cp_size - rank - 1,
        seq_blocksize=seq_blocksize,
        max_seqlen_q=seq_blocksize,
    )

    # Perform flashmask attention with startend_row_indices
    output, log_sum_exp = flashmask_attention(
        query,
        key_gathered,
        value_gathered,
        startend_row_indices=startend_row_indices,
        causal=causal,
        return_softmax_lse=True,
        training=is_training,
    )

    paddle.base.core.nvprof_nvtx_pop()
    return output, log_sum_exp, startend_row_indices


def cp_flashmask_allgatherkv_balance_backward(
    query, key, value, startend_row_indices, output, log_sum_exp, output_grad, group, causal
):
    """
    Backward pass of context parallel flashmask attention with balanced all-gather strategy.

    This function implements the backward pass of flashmask attention with context parallelism,
    computing gradients for query, key, and value tensors.

    Args:
        query (paddle.Tensor): Query tensor
        key (paddle.Tensor): Key tensor
        value (paddle.Tensor): Value tensor
        startend_row_indices (paddle.Tensor): Processed startend_row_indices
        output (paddle.Tensor): Forward pass output
        log_sum_exp (paddle.Tensor): Log-sum-exp from forward pass
        output_grad (paddle.Tensor): Gradient of output
        group (paddle.distributed.Group): Communication group
        causal (bool): Whether causal attention was used

    Returns:
        tuple: (query_grad, key_grad, value_grad)
    """
    paddle.base.core.nvprof_nvtx_push("cp_flashmask_allgatherkv_balance_backward")

    cp_size = group.world_size

    # All-gather key and value tensors (same as forward pass)
    key_gathered = all_gather_balance(key, axis=1, group=group)
    value_gathered = all_gather_balance(value, axis=1, group=group)
    
    x_np = startend_row_indices.numpy()
    rank = dist.get_rank()
    np.savetxt(f'tensor_{rank}.txt', x_np.reshape(-1, x_np.shape[-1]), fmt='%d')
    print(f'rank:{rank} ,qshape:{query.shape}, kshape:{key.shape}, vshape:{value.shape}')

    if paddle.get_flags(["FLAGS_cudnn_deterministic"])["FLAGS_cudnn_deterministic"]:
        fa_version = 2
    else:
        fa_version = paddle.base.framework.get_flags(["FLAGS_flash_attn_version"])["FLAGS_flash_attn_version"]
    if fa_version == 2:
        # Create seed offset tensor (required for gradient computation)
        seed_offset = paddle.zeros(shape=[query.shape[1], query.shape[2]], dtype=paddle.int64)

        # Compute gradients using flashmask attention backward pass
        query_grad, key_grad_gathered, value_grad_gathered = paddle._C_ops.flashmask_attention_grad(
            query,
            key_gathered,
            value_gathered,
            startend_row_indices,
            output,
            log_sum_exp,
            seed_offset,
            output_grad,
            0.0,  # dropout probability
            causal,
        )
    elif fa_version == 3:
        query_grad, key_grad_gathered, value_grad_gathered = paddle._C_ops.flashmask_attention_v2_grad(
            query,
            key_gathered,
            value_gathered,
            output,
            log_sum_exp,
            startend_row_indices,
            # None,
            output_grad,
            query.shape[-1] ** (-0.5),
            False,
        )
    else:
        raise ValueError(f"FlashAttention version {fa_version} is not supported.")
    
    paddle.device.synchronize()
    
    rank = group.rank
    print(f"rank:{rank} pass backward")

    # Reduce-scatter key and value gradients
    key_grad = reduce_scatter_any_axis_balance(key_grad_gathered, axis=1, group=group)
    print(f"rank:{rank} pass key scatter")
    value_grad = reduce_scatter_any_axis_balance(value_grad_gathered, axis=1, group=group)
    print(f"rank:{rank} pass value scatter")

    paddle.base.core.nvprof_nvtx_pop()
    return query_grad, key_grad, value_grad

class FlashMaskContextParallel(PyLayer):
    """
    FlashMask attention with context parallelism implementation.

    This class implements flashmask attention with context parallelism (CP) using PyLayer
    for automatic differentiation. CP partitions tensors along the sequence dimension
    to enable long-context LLMs in a distributed fashion.

    The implementation uses the DualChunkSwap strategy to ensure load balancing
    across CP ranks by processing chunks from both ends of the sequence.
    """

    @staticmethod
    def forward(
        ctx,
        query,
        key,
        value,
        startend_row_indices,
        fixed_seed_offset=None,
        dropout=0.0,
        causal=False,
        training=True,
        mode="allgather_kv",
    ):
        """
        Forward pass of FlashMask attention with context parallelism.

        Args:
            ctx: Context object for saving information for backward pass
            query (paddle.Tensor): Query tensor, pre-divided by CP size
            key (paddle.Tensor): Key tensor, pre-divided by CP size
            value (paddle.Tensor): Value tensor, pre-divided by CP size
            startend_row_indices (paddle.Tensor): Row indices for attention mask
            fixed_seed_offset (paddle.Tensor, optional): Fixed seed offset for dropout
            dropout (float): Dropout probability
            causal (bool): Whether to use causal attention
            training (bool): Whether in training mode
            mode (str): Attention mode, currently supports "allgather_kv"

        Returns:
            paddle.Tensor: Attention output

        Raises:
            NotImplementedError: If dropout > 0.0 or causal=True
            AssertionError: If query sequence length is not divisible by 2
        """
        # Validate input parameters
        if dropout > 0.0:
            raise NotImplementedError("Dropout is not supported in FlashMask context parallel yet.")

        if causal:
            raise NotImplementedError("FlashMaskContextParallel does not support causal=True yet.")

        if fixed_seed_offset is not None:
            raise NotImplementedError("Fixed seed offset is not supported yet.")

        # Get communication group
        hcg = fleet.get_hybrid_communicate_group()
        group = hcg.get_context_parallel_group()

        # Validate query sequence length for DualChunkSwap strategy
        assert query.shape[1] % 2 == 0, (
            f"Query sequence length must be divisible by 2. "
            f"FlashMaskContextParallel uses DualChunkSwap strategy for load balancing. "
            f"Current query sequence length: {query.shape[1]}"
        )

        # Perform forward pass
        output, log_sum_exp, startend_row_indices = cp_flashmask_allgatherkv_balance_forward(
            query, key, value, startend_row_indices, group, causal, training
        )

        # Save tensors for backward pass
        ctx.save_for_backward(query, key, value, output, log_sum_exp, startend_row_indices)
        ctx.group = group
        ctx.causal = causal

        return output

    @staticmethod
    def backward(ctx, output_grad):
        """
        Backward pass of FlashMask attention with context parallelism.

        Args:
            ctx: Context object with saved information
            output_grad (paddle.Tensor): Gradient of output

        Returns:
            tuple: Gradients for all input arguments
        """
        # Retrieve saved tensors
        query, key, value, output, log_sum_exp, startend_row_indices = ctx.saved_tensor()
        group = ctx.group
        causal = ctx.causal

        # Compute gradients
        query_grad, key_grad, value_grad = cp_flashmask_allgatherkv_balance_backward(
            query, key, value, startend_row_indices, output, log_sum_exp, output_grad, group, causal
        )

        return query_grad, key_grad, value_grad


def flashmask_attention_cp(
    query,
    key,
    value,
    startend_row_indices,
    fixed_seed_offset=None,
    dropout=0.0,
    causal=False,
    training=True,
    mode="allgather_kv",
):
    """
    FlashMask attention with context parallelism - public API.

    This is the main entry point for using FlashMask attention with context parallelism.
    It provides a convenient interface that wraps the FlashMaskContextParallel PyLayer.

    Args:
        query (paddle.Tensor): Query tensor with shape [batch, seq_len/n, num_heads, head_dim]
        key (paddle.Tensor): Key tensor with shape [batch, seq_len/n, num_heads, head_dim]
        value (paddle.Tensor): Value tensor with shape [batch, seq_len/n, num_heads, head_dim]
        startend_row_indices (paddle.Tensor): Row indices for attention mask
        fixed_seed_offset (paddle.Tensor, optional): Fixed seed offset for dropout
        dropout (float, optional): Dropout probability. Defaults to 0.0
        causal (bool, optional): Whether to use causal attention. Defaults to False
        training (bool, optional): Whether in training mode. Defaults to True
        mode (str, optional): Attention mode. Defaults to "allgather_kv"

    Returns:
        paddle.Tensor: Attention output with shape [batch, seq_len/n, num_heads, head_dim]

    Example:
        ```python
        # Initialize tensors (assuming context parallelism is set up)
        query = paddle.randn([2, 512, 8, 64])  # [batch, seq_len/n, heads, head_dim]
        key = paddle.randn([2, 512, 8, 64])    # [batch, seq_len/n, heads, head_dim]
        value = paddle.randn([2, 512, 8, 64])  # [batch, seq_len/n, heads, head_dim]
        mask_indices = paddle.randint(0, 1024, [100, 2])

        # Apply FlashMask attention with context parallelism
        output = flashmask_attention_cp(
            query=query,
            key=key,
            value=value,
            startend_row_indices=mask_indices,
            training=True
        )
        ```
    """
    output = FlashMaskContextParallel.apply(
        query,
        key,
        value,
        startend_row_indices,
        fixed_seed_offset,
        dropout,
        causal,
        training,
        mode,
    )
    return output